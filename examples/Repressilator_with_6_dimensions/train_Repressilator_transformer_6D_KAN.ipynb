{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPjTnCG-v2Z6",
        "outputId": "67abe716-e41f-47b7-9312-c7e32a9fbcb2"
      },
      "source": [
        "\"\"\"\n",
        "Notebook for training the transformer model for the Rossler system.\n",
        "=====\n",
        "Distributed by: Notre Dame SCAI Lab (MIT Liscense)\n",
        "- Associated publication:\n",
        "url: https://arxiv.org/abs/2010.03957\n",
        "doi:\n",
        "github: https://github.com/zabaras/transformer-physx\n",
        "=====\n",
        "\"\"\"\n",
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jul  3 21:17:00 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjCxpDBPVPdq"
      },
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adPNHn3_Q25_"
      },
      "source": [
        "Use pip to install from [PyPI](https://pypi.org/project/trphysx/).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvm518_H3AK7",
        "outputId": "d3907be9-d5ed-4747-d49d-4cfe3bcfa59b"
      },
      "source": [
        "!pip install trphysx==0.0.7"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting trphysx==0.0.7\n",
            "  Downloading trphysx-0.0.7-py3-none-any.whl (137 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/137.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from trphysx==0.0.7) (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from trphysx==0.0.7) (3.15.4)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from trphysx==0.0.7) (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from trphysx==0.0.7) (1.25.2)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from trphysx==0.0.7) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->trphysx==0.0.7) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->trphysx==0.0.7) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->trphysx==0.0.7) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->trphysx==0.0.7) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->trphysx==0.0.7) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->trphysx==0.0.7) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->trphysx==0.0.7) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->trphysx==0.0.7) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->trphysx==0.0.7) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->trphysx==0.0.7) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->trphysx==0.0.7) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->trphysx==0.0.7) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->trphysx==0.0.7) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.7.0->trphysx==0.0.7)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.7.0->trphysx==0.0.7)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.7.0->trphysx==0.0.7)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.7.0->trphysx==0.0.7)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.7.0->trphysx==0.0.7)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.7.0->trphysx==0.0.7)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.7.0->trphysx==0.0.7)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.7.0->trphysx==0.0.7)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.7.0->trphysx==0.0.7)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.7.0->trphysx==0.0.7)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.7.0->trphysx==0.0.7)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->trphysx==0.0.7) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7.0->trphysx==0.0.7)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->trphysx==0.0.7) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.0->trphysx==0.0.7) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.0->trphysx==0.0.7) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, trphysx\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 trphysx-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoBoGx0J0LtZ"
      },
      "source": [
        "First mount google drive and clone transformer physx repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3K0hSst0b2Ak",
        "outputId": "f1f8754b-e892-408a-d34c-d596644e53cb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJL720VLw46q",
        "outputId": "9572dcae-7dcf-423d-e266-fa8ce611cc55"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/\n",
        "%mkdir -p transformer_physx/Repressilator_KAN\n",
        "%cd transformer_physx/Repressilator_KAN"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive\n",
            "/content/gdrive/MyDrive/transformer_physx/Repressilator_KAN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlsHPvFNVUnQ"
      },
      "source": [
        "## Downloading Data and Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MK_h8wF0Rr4"
      },
      "source": [
        "Now lets download the training and validation data for the Rossler system. Info on wget from [Google drive](https://stackoverflow.com/questions/37453841/download-a-file-from-google-drive-using-wget). This will eventually be update to zenodo repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NtZ02zD0EKo"
      },
      "source": [
        "!mkdir data"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cU702uo6xIQQ",
        "outputId": "87324813-c7f6-4177-fcea-af80ff8dcfe8"
      },
      "source": [
        "# Updated it with the links from your google drive where generated data is residing\n",
        "!wget 'https://docs.google.com/uc?export=download&id=1hFfA0kYz_M11Bp-vVtQqNKbeZO-XGrr9' -O ./data/Repressilator_training.hdf5\n",
        "!wget 'https://docs.google.com/uc?export=download&id=1B0_A_Nwp0Yn80tDf9FcLD67PZUyJHKPi' -O ./data/Repressilator_valid.hdf5"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-03 21:21:36--  https://docs.google.com/uc?export=download&id=1hFfA0kYz_M11Bp-vVtQqNKbeZO-XGrr9\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.2.139, 142.251.2.100, 142.251.2.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.2.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1hFfA0kYz_M11Bp-vVtQqNKbeZO-XGrr9&export=download [following]\n",
            "--2024-07-03 21:21:36--  https://drive.usercontent.google.com/download?id=1hFfA0kYz_M11Bp-vVtQqNKbeZO-XGrr9&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.251.2.132, 2607:f8b0:4023:c0d::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.251.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 101533200 (97M) [application/octet-stream]\n",
            "Saving to: ‘./data/Repressilator_training.hdf5’\n",
            "\n",
            "./data/Repressilato 100%[===================>]  96.83M  55.6MB/s    in 1.7s    \n",
            "\n",
            "2024-07-03 21:21:41 (55.6 MB/s) - ‘./data/Repressilator_training.hdf5’ saved [101533200/101533200]\n",
            "\n",
            "--2024-07-03 21:21:41--  https://docs.google.com/uc?export=download&id=1B0_A_Nwp0Yn80tDf9FcLD67PZUyJHKPi\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.2.139, 142.251.2.100, 142.251.2.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.2.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1B0_A_Nwp0Yn80tDf9FcLD67PZUyJHKPi&export=download [following]\n",
            "--2024-07-03 21:21:41--  https://drive.usercontent.google.com/download?id=1B0_A_Nwp0Yn80tDf9FcLD67PZUyJHKPi&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.251.2.132, 2607:f8b0:4023:c0d::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.251.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3173376 (3.0M) [application/octet-stream]\n",
            "Saving to: ‘./data/Repressilator_valid.hdf5’\n",
            "\n",
            "./data/Repressilato 100%[===================>]   3.03M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-07-03 21:21:43 (20.8 MB/s) - ‘./data/Repressilator_valid.hdf5’ saved [3173376/3173376]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vKlS8OF-aoT"
      },
      "source": [
        "Next lets download a pretrained embedding model. You can replace with with your own if you want."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNZkweD0-hu1",
        "outputId": "0621ab03-de5e-4f90-9821-089d14cf4fdb"
      },
      "source": [
        "#https://drive.google.com/file/d/1HR-J3hYZVDAGddzuEvKYUiJstoa4Vsvq/view?usp=sharing\n",
        "!wget 'https://docs.google.com/uc?export=download&id=1HR-J3hYZVDAGddzuEvKYUiJstoa4Vsvq' -O ./embedding_Repressilator300.pth\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-03 21:21:47--  https://docs.google.com/uc?export=download&id=1HR-J3hYZVDAGddzuEvKYUiJstoa4Vsvq\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.2.139, 142.251.2.100, 142.251.2.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.2.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1HR-J3hYZVDAGddzuEvKYUiJstoa4Vsvq&export=download [following]\n",
            "--2024-07-03 21:21:47--  https://drive.usercontent.google.com/download?id=1HR-J3hYZVDAGddzuEvKYUiJstoa4Vsvq&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.251.2.132, 2607:f8b0:4023:c0d::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.251.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 288770 (282K) [application/octet-stream]\n",
            "Saving to: ‘./embedding_Repressilator300.pth’\n",
            "\n",
            "./embedding_Repress 100%[===================>] 282.00K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-07-03 21:21:48 (3.95 MB/s) - ‘./embedding_Repressilator300.pth’ saved [288770/288770]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgRznlNJVH0Y"
      },
      "source": [
        "# Transformer-PhysX Repressilator System"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDVeeJHn11Ir"
      },
      "source": [
        "The Rossler system is **not** a prebuilt example in trphysx thus we need to create four classes for our numerical example:\n",
        "- *Config class* - Embedding and mainly transformer architecture parameters\n",
        "- *Embedding class* - Convert states into embedding vector\n",
        "- *Visualization class* - Visualize predictions\n",
        "- *Transformer dataset class* - Create dataset for training\n",
        "\n",
        "Fortunately trphysx has base classes for all of these that contain both useful methods and as well as abstract declarations to help guide you."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ZiyaoLi/fast-kan\n",
        "%pwd\n",
        "%cd fast-kan\n",
        "%pip install .\n",
        "%cd ..\n",
        "%pwd"
      ],
      "metadata": {
        "id": "6Pkql4zm457h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "ef759185-f9a3-4b7f-f726-6309946dae0d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fast-kan'...\n",
            "remote: Enumerating objects: 202, done.\u001b[K\n",
            "remote: Counting objects: 100% (78/78), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 202 (delta 53), reused 61 (delta 45), pack-reused 124\u001b[K\n",
            "Receiving objects: 100% (202/202), 420.58 KiB | 1.47 MiB/s, done.\n",
            "Resolving deltas: 100% (92/92), done.\n",
            "/content/gdrive/MyDrive/transformer_physx/Repressilator_KAN/fast-kan\n",
            "Processing /content/gdrive/MyDrive/transformer_physx/Repressilator_KAN/fast-kan\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fastkan==0.0.1) (1.25.2)\n",
            "Building wheels for collected packages: fastkan\n",
            "  Building wheel for fastkan (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastkan: filename=fastkan-0.0.1-py3-none-any.whl size=11559 sha256=6c1df72273b4e3fc3c57f956f884b8cff4ba1e88c80a90b9e0ab21720de6fbc3\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/d0/4a/8b91a17288fc452d53c1c5255b19378d7b9b5fdc7925f2dbe6\n",
            "Successfully built fastkan\n",
            "Installing collected packages: fastkan\n",
            "  Attempting uninstall: fastkan\n",
            "    Found existing installation: fastkan 0.0.1\n",
            "    Uninstalling fastkan-0.0.1:\n",
            "      Successfully uninstalled fastkan-0.0.1\n",
            "Successfully installed fastkan-0.0.1\n",
            "/content/gdrive/MyDrive/transformer_physx/Repressilator_KAN\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive/transformer_physx/Repressilator_KAN'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNGVZQ-o1gsH"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import logging\n",
        "import h5py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "from typing import Dict, List, Tuple\n",
        "# Torch imports\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "# Trphysx imports\n",
        "from trphysx.config import HfArgumentParser\n",
        "from trphysx.config.args import ModelArguments, TrainingArguments, DataArguments, ArgUtils\n",
        "from trphysx.embedding import EmbeddingModel\n",
        "from trphysx.config.configuration_phys import PhysConfig\n",
        "from trphysx.data_utils.dataset_phys import PhysicalDataset\n",
        "from trphysx.transformer import PhysformerTrain, PhysformerGPT2\n",
        "from trphysx.utils.trainer import Trainer\n",
        "##importing KAN\n",
        "from fastkan import FastKAN as KAN\n",
        "\n",
        "\n",
        "Tensor = torch.Tensor\n",
        "TensorTuple = Tuple[torch.Tensor]\n",
        "FloatTuple = Tuple[float]\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "logging.basicConfig(\n",
        "    filename='logging.log',\n",
        "    filemode='a',\n",
        "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    level=logging.DEBUG,\n",
        "    force=True,)\n",
        "\n",
        "\n",
        "logger.info(\"Torch device: {}\")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"#####################################################################################\")\n",
        "logger.info(\"A new training is started\")"
      ],
      "metadata": {
        "id": "0WI8eW1pWR_-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEft0ltg4swx"
      },
      "source": [
        "Set training arguments. For running this outside of a notebook, you would use \"sys.argv\" and then no argument when you parse args into dataclasses. This would allow the use of command line parameters as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8R8QQ0cj4qR9"
      },
      "source": [
        "argv = []\n",
        "argv = argv + [\"--init_name\", \"repressilator\"]\n",
        "argv = argv + [\"--embedding_file_or_path\", \"./embedding_Repressilator300.pth\"]\n",
        "argv = argv + [\"--training_h5_file\",\"./data/Repressilator_training.hdf5\"]\n",
        "argv = argv + [\"--eval_h5_file\",\"./data/Repressilator_valid.hdf5\"]\n",
        "argv = argv + [\"--train_batch_size\", \"32\"]\n",
        "argv = argv + [\"--lr\",\"0.0001\"]\n",
        "argv = argv + [\"--stride\", \"32\"]\n",
        "argv = argv + [\"--block_size\", \"32\"]\n",
        "argv = argv + [\"--n_train\", \"10240\"]\n",
        "argv = argv + [\"--n_eval\", \"32\"]\n",
        "argv = argv + [\"--save_steps\", \"25\"]\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75HXWsO0huoc"
      },
      "source": [
        "## Repressilator Config Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EoTZ2Gthyuj"
      },
      "source": [
        "class RepressilatorConfig(PhysConfig):\n",
        "    \"\"\"\n",
        "    This is the configuration class for the modeling of the Repressilator system.\n",
        "    \"\"\"\n",
        "\n",
        "    model_type = \"repressilator\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_ctx=32,\n",
        "        n_embd=32,\n",
        "        n_layer=4,\n",
        "        n_head=4, # n_head must be a factor of n_embd\n",
        "        state_dims=[6],\n",
        "        activation_function=\"gelu_new\",\n",
        "        initializer_range=0.02,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(\n",
        "            n_ctx=n_ctx,\n",
        "            n_embd=n_embd,\n",
        "            n_layer=n_layer,\n",
        "            n_head=n_head,\n",
        "            state_dims=state_dims,\n",
        "            activation_function=activation_function,\n",
        "            initializer_range=initializer_range,\n",
        "            **kwargs\n",
        "        )"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKUsluH6h48P"
      },
      "source": [
        "## Embedding Neural Network Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hHxgYUTmRxH"
      },
      "source": [
        "Note here that we only need a subset of the methods needed for training the embedding model namely we just need \"embed\" and \"recover\" methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quAIE8Zgh-gF"
      },
      "source": [
        "class RepressilatorEmbedding(EmbeddingModel):\n",
        "    \"\"\"Embedding model for the Repressilator ODE system\n",
        "\n",
        "    Args:\n",
        "        config (PhysConfig) Configuration class with transformer/embedding parameters\n",
        "    \"\"\"\n",
        "    model_name = \"embedding_repressilator\"\n",
        "\n",
        "    def __init__(self, config: PhysConfig) -> None:\n",
        "        \"\"\"Constructor method\n",
        "        \"\"\"\n",
        "        super().__init__(config)\n",
        "\n",
        "        hidden_states = int(abs(config.state_dims[0] - config.n_embd)/2) + 1\n",
        "        hidden_states = 100\n",
        "\n",
        "        self.observableNet = nn.Sequential(\n",
        "            #nn.Linear(config.state_dims[0], hidden_states),\n",
        "            #nn.ReLU(),\n",
        "            #nn.Linear(hidden_states, config.n_embd),\n",
        "            KAN([config.state_dims[0], hidden_states,config.n_embd]),\n",
        "            nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon),\n",
        "            nn.Dropout(config.embd_pdrop)\n",
        "        )\n",
        "\n",
        "        self.recoveryNet = KAN([config.n_embd, hidden_states,config.state_dims[0]])\n",
        "        # Learned koopman operator\n",
        "        # Learns skew-symmetric matrix with a diagonal\n",
        "        self.obsdim = config.n_embd\n",
        "        self.kMatrixDiag = nn.Parameter(torch.linspace(1, 0, config.n_embd))\n",
        "\n",
        "        xidx = []\n",
        "        yidx = []\n",
        "        for i in range(1, 3):\n",
        "            yidx.append(np.arange(i, config.n_embd))\n",
        "            xidx.append(np.arange(0, config.n_embd-i))\n",
        "\n",
        "        self.xidx = torch.LongTensor(np.concatenate(xidx))\n",
        "        self.yidx = torch.LongTensor(np.concatenate(yidx))\n",
        "        self.kMatrixUT = nn.Parameter(0.1*torch.rand(self.xidx.size(0)))\n",
        "        # Normalization occurs inside the model\n",
        "        self.register_buffer('mu', torch.tensor([0., 0., 0., 0., 0., 0.]))\n",
        "        self.register_buffer('std', torch.tensor([1., 1., 1., 1., 1., 1.]))\n",
        "        print('Number of embedding parameters: {}'.format( super().num_parameters ))\n",
        "\n",
        "    def embed(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"Embeds tensor of state variables to Koopman observables\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): [B, 6] input feature tensor\n",
        "\n",
        "        Returns:\n",
        "            (Tensor): [B, config.n_embd] Koopman observables\n",
        "        \"\"\"\n",
        "        x = self._normalize(x)\n",
        "        g = self.observableNet(x)\n",
        "        return g\n",
        "\n",
        "    def recover(self, g: Tensor) -> Tensor:\n",
        "        \"\"\"Recovers feature tensor from Koopman observables\n",
        "\n",
        "        Args:\n",
        "            g (Tensor): [B, config.n_embd] Koopman observables\n",
        "\n",
        "        Returns:\n",
        "            (Tensor): [B, 6] Physical feature tensor\n",
        "        \"\"\"\n",
        "        out = self.recoveryNet(g)\n",
        "        x = self._unnormalize(out)\n",
        "        return x\n",
        "\n",
        "    def _normalize(self, x: Tensor) -> Tensor:\n",
        "        return (x - self.mu.unsqueeze(0))/self.std.unsqueeze(0)\n",
        "\n",
        "    def _unnormalize(self, x: Tensor) -> Tensor:\n",
        "        return self.std.unsqueeze(0)*x + self.mu.unsqueeze(0)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE__niItjdkM"
      },
      "source": [
        "## Repressilator Visualization Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEpJyca8jge5"
      },
      "source": [
        "import matplotlib as mpl\n",
        "mpl.use('agg')\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from mpl_toolkits.mplot3d.art3d import Line3DCollection\n",
        "from matplotlib.patches import Rectangle\n",
        "from matplotlib.legend_handler import HandlerBase\n",
        "from trphysx.viz import Viz\n",
        "\n",
        "# Interface to LineCollection:\n",
        "def _colorline3d(x, y, z, t=None, cmap=plt.get_cmap('viridis'), linewidth=1, alpha=1.0, ax=None):\n",
        "    '''\n",
        "    Plot a colored line with coordinates x and y\n",
        "    Optionally specify colors in the array z\n",
        "    Optionally specify a colormap, a norm function and a line width\n",
        "    https://stackoverflow.com/questions/52884221/how-to-plot-a-matplotlib-line-plot-using-colormap\n",
        "    '''\n",
        "    # Default colors equally spaced on [0,1]:\n",
        "    if t is None:\n",
        "        t = np.linspace(0.25, 1.0, len(x))\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "\n",
        "    points = np.array([x, y, z]).T.reshape(-1, 1, 3)\n",
        "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
        "\n",
        "    colors = np.array([cmap(i) for i in t])\n",
        "    lc = Line3DCollection(segments, colors=colors, linewidth=linewidth,  alpha=alpha)\n",
        "    ax.add_collection(lc)\n",
        "    ax.scatter(x, y, z, c=colors, marker='*', alpha=alpha) #Adding line markers\n",
        "\n",
        "class HandlerColormap(HandlerBase):\n",
        "    def __init__(self, cmap, num_stripes=8, **kw):\n",
        "        HandlerBase.__init__(self, **kw)\n",
        "        self.cmap = cmap\n",
        "        self.num_stripes = num_stripes\n",
        "    def create_artists(self, legend, orig_handle,\n",
        "                       xdescent, ydescent, width, height, fontsize, trans):\n",
        "        stripes = []\n",
        "        for i in range(self.num_stripes):\n",
        "            s = Rectangle([xdescent + i * width / self.num_stripes, ydescent],\n",
        "                          width / self.num_stripes,\n",
        "                          height,\n",
        "                          fc=self.cmap((2 * i + 1) / (2 * self.num_stripes)),\n",
        "                          transform=trans)\n",
        "            stripes.append(s)\n",
        "        return stripes\n",
        "\n",
        "class RepressilatorViz(Viz):\n",
        "    \"\"\"Visualization class for Rosler ODE\n",
        "\n",
        "    Args:\n",
        "        plot_dir (str, optional): Directory to save visualizations in. Defaults to None.\n",
        "    \"\"\"\n",
        "    def __init__(self, plot_dir:str = None) -> None:\n",
        "        super().__init__(plot_dir=plot_dir)\n",
        "\n",
        "    def plotPrediction(self,\n",
        "        y_pred: Tensor,\n",
        "        y_target: Tensor,\n",
        "        plot_dir: str = None,\n",
        "        epoch: int = None,\n",
        "        pid: int = 0,\n",
        "        nsteps: int = 256\n",
        "    ) -> None:\n",
        "        \"\"\"Plots a 3D line of a single Repressilator prediction\n",
        "\n",
        "        Args:\n",
        "            y_pred (Tensor): [T, 3] Prediction tensor.\n",
        "            y_target (Tensor): [T, 3] Target tensor.\n",
        "            plot_dir (str, optional): Directory to save figure, overrides plot_dir one if provided. Defaults to None.\n",
        "            epoch (int, optional): Current epoch, used for file name. Defaults to None.\n",
        "            pid (int, optional): Optional plotting id for indexing file name manually. Defaults to 0.\n",
        "            nsteps (int, optional): Number of steps to plot. Defaults to 256.\n",
        "        \"\"\"\n",
        "        # Convert to numpy array\n",
        "        y_pred = y_pred[:nsteps].detach().cpu().numpy()\n",
        "        y_target = y_target[:nsteps].detach().cpu().numpy()\n",
        "\n",
        "        plt.close('all')\n",
        "        mpl.rcParams['font.family'] = ['serif'] # default is sans-serif\n",
        "        mpl.rcParams['figure.dpi'] = 300\n",
        "        # rc('text', usetex=True)\n",
        "        # Set up figure\n",
        "        fig = plt.figure(figsize=(10, 10))\n",
        "        ax = fig.add_subplot(1, 1, 1, projection='3d')\n",
        "\n",
        "        cmaps = [plt.get_cmap(\"Reds\"), plt.get_cmap(\"Blues\")]\n",
        "        _colorline3d(y_pred[:,0], y_pred[:,1], y_pred[:,2], cmap=cmaps[0], ax=ax)\n",
        "        _colorline3d(y_target[:,0], y_target[:,1], y_target[:,2], cmap=cmaps[1], ax=ax)\n",
        "\n",
        "        ax.set_xlim([np.amin(y_target[:,0])-5, np.amax(y_target[:,0])+5])\n",
        "        ax.set_ylim([np.amin(y_target[:,1])-5, np.amax(y_target[:,1])+5])\n",
        "        ax.set_zlim([np.amin(y_target[:,2])-5, np.amax(y_target[:,2])+5])\n",
        "\n",
        "        cmap_handles = [Rectangle((0, 0), 1, 1) for _ in cmaps]\n",
        "        handler_map = dict(zip(cmap_handles,\n",
        "                            [HandlerColormap(cm, num_stripes=8) for cm in cmaps]))\n",
        "        # Create custom legend with color map rectangels\n",
        "        ax.legend(handles=cmap_handles, labels=['Prediction','Target'], handler_map=handler_map, loc='upper right', framealpha=0.95)\n",
        "\n",
        "        if(not epoch is None):\n",
        "            file_name = 'repressilatorPred{:d}_{:d}'.format(pid, epoch)\n",
        "        else:\n",
        "            file_name = 'repressilatorPred{:d}'.format(pid)\n",
        "\n",
        "        self.saveFigure(plot_dir, file_name)\n",
        "\n",
        "    def plotMultiPrediction(self,\n",
        "        y_pred: Tensor,\n",
        "        y_target: Tensor,\n",
        "        plot_dir: str = None,\n",
        "        epoch: int = None,\n",
        "        pid: int = 0,\n",
        "        nplots: int = 2\n",
        "    ) -> None:\n",
        "        \"\"\"Plots the 3D lines of multiple Repressilator predictions\n",
        "\n",
        "        Args:\n",
        "            y_pred (Tensor): [T, 3] Prediction tensor.\n",
        "            y_target (Tensor): [T, 3] Target tensor.\n",
        "            plot_dir (str, optional): Directory to save figure, overrides plot_dir one if provided. Defaults to None.\n",
        "            epoch (int, optional): Current epoch, used for file name. Defaults to None.\n",
        "            pid (int, optional): Optional plotting id for indexing file name manually, Defaults to 0.\n",
        "            nplots (int, optional): Number of cases to plot, Defaults to 2.\n",
        "        \"\"\"\n",
        "        assert y_pred.size(0) >= nplots, 'Number of provided predictions is less than the requested number of subplots'\n",
        "        assert y_target.size(0) >= nplots, 'Number of provided targets is less than the requested number of subplots'\n",
        "        # Convert to numpy array\n",
        "        y_pred = y_pred.detach().cpu().numpy()\n",
        "        y_target = y_target.detach().cpu().numpy()\n",
        "\n",
        "        plt.close('all')\n",
        "        mpl.rcParams['font.family'] = ['serif']  # default is sans-serif\n",
        "        mpl.rcParams['figure.dpi'] = 300\n",
        "        # rc('text', usetex=True)\n",
        "        # Set up figure\n",
        "        fig, ax = plt.subplots(1, nplots, figsize=(6*nplots, 6), subplot_kw={'projection': '3d'})\n",
        "        plt.subplots_adjust(wspace=0.025)\n",
        "\n",
        "        cmaps = [plt.get_cmap(\"Reds\"), plt.get_cmap(\"Blues\")]\n",
        "        for i in range(nplots):\n",
        "            _colorline3d(y_pred[i, :, 0], y_pred[i, :, 1], y_pred[i, :, 2], cmap=cmaps[0], ax=ax[i], alpha=0.6)\n",
        "            _colorline3d(y_target[i, :, 0], y_target[i, :, 1], y_target[i, :, 2], cmap=cmaps[1], ax=ax[i], alpha=0.6)\n",
        "\n",
        "            ax[i].set_xlim([np.amin(y_target[:,0])-5, np.amax(y_target[:,0])+5])\n",
        "            ax[i].set_ylim([np.amin(y_target[:,1])-5, np.amax(y_target[:,1])+5])\n",
        "            ax[i].set_zlim([np.amin(y_target[:,2])-5, np.amax(y_target[:,2])+5])\n",
        "\n",
        "            ax[i].set_xlabel('x', fontsize=14)\n",
        "            ax[i].set_ylabel('y', fontsize=14)\n",
        "            ax[i].set_zlabel('z', fontsize=14)\n",
        "        cmap_handles = [Rectangle((0, 0), 1, 1) for _ in cmaps]\n",
        "        handler_map = dict(zip(cmap_handles,\n",
        "                               [HandlerColormap(cm, num_stripes=10) for cm in cmaps]))\n",
        "\n",
        "        # Create custom legend with color map rectangels\n",
        "        ax[-1].legend(handles=cmap_handles, labels=['Prediction', 'Target'], handler_map=handler_map, loc='upper right',\n",
        "                  framealpha=0.95)\n",
        "\n",
        "        if epoch is not None:\n",
        "            file_name = 'repressilatorMultiPred{:d}_{:d}'.format(pid, epoch)\n",
        "        else:\n",
        "            file_name = 'repressilatorMultiPred{:d}'.format(pid)\n",
        "\n",
        "        self.saveFigure(plot_dir, file_name)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnuHqNRCiC4n"
      },
      "source": [
        "## Transformer Dataset Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAzXd6HSmgxY"
      },
      "source": [
        "Similar to the  built in examples to create a data-set for a physics transformer the \"embed_data\" method needs to be overloaded which tells trphysx how to transformer states into an embedded vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G90ZHyNiOTX"
      },
      "source": [
        "class RepressilatorDataset(PhysicalDataset):\n",
        "    \"\"\"Dataset for the Repressilator numerical example\n",
        "    \"\"\"\n",
        "    def embed_data(self, h5_file: h5py.File, embedder: EmbeddingModel) -> None:\n",
        "        \"\"\"Embeds Repressilator data into a 1D vector representation for the transformer.\n",
        "\n",
        "        Args:\n",
        "            h5_file (h5py.File): HDF5 file object of raw data\n",
        "            embedder (EmbeddingModel): Embedding neural network\n",
        "        \"\"\"\n",
        "        # Iterate through stored time-series\n",
        "        samples = 0\n",
        "        for key in h5_file.keys():\n",
        "            data_series = torch.Tensor(h5_file[key]).to(embedder.devices[0]).view([-1] + embedder.input_dims)\n",
        "            with torch.no_grad():\n",
        "                embedded_series = embedder.embed(data_series).cpu()\n",
        "            # Stride over time-series\n",
        "            for i in range(0, data_series.size(0) - self.block_size + 1,\n",
        "                           self.stride):  # Truncate in block of block_size\n",
        "                data_series0 = embedded_series[i: i + self.block_size]\n",
        "                self.examples.append(data_series0)\n",
        "\n",
        "                if self.eval:\n",
        "                    self.states.append(data_series[i: i + self.block_size].cpu())\n",
        "\n",
        "            samples = samples + 1\n",
        "            if (self.ndata > 0 and samples >= self.ndata):  # If we have enough time-series samples break loop\n",
        "                break\n",
        "\n",
        "        logger.info(\n",
        "            'Collected {:d} time-series from hdf5 file. Total of {:d} time-series.'.format(samples, len(self.examples))\n",
        "            )"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "=====\n",
        "Distributed by: Notre Dame SCAI Lab (MIT Liscense)\n",
        "- Associated publication:\n",
        "url: https://arxiv.org/abs/2010.03957\n",
        "doi:\n",
        "github: https://github.com/zabaras/transformer-physx\n",
        "=====\n",
        "\"\"\"\n",
        "import os\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional, Tuple #Needs python 3.8 for literal\n",
        "\n",
        "HOME = os.getcwd()\n",
        "INITS = ['lorenz', 'cylinder', 'grayscott']\n",
        "\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
        "    \"\"\"\n",
        "    init_name: str = field(\n",
        "        default='lorenz', metadata={\"help\": \"Used as a global default initialization token for different experiments.\"}\n",
        "    )\n",
        "    model_name: str = field(\n",
        "        default=None, metadata={\"help\": \"The name model of the transformer model\"},\n",
        "    )\n",
        "    config_name: str = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
        "    )\n",
        "    embedding_name: str = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained embedding model name\"}\n",
        "    )\n",
        "    embedding_file_or_path: str = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained embedding model path\"}\n",
        "    )\n",
        "    transformer_file_or_path: str = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained transformer model path\"}\n",
        "    )\n",
        "    viz_name: str = field(\n",
        "        default=None, metadata={\"help\": \"Visualization class name\"}\n",
        "    )\n",
        "\n",
        "@dataclass\n",
        "class DataArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to training and evaluation data.\n",
        "    \"\"\"\n",
        "    n_train: int = field(\n",
        "        default=2048, metadata={\"help\": \"Number of training time-series to use\"}\n",
        "    )\n",
        "    n_eval: int = field(\n",
        "        default=256, metadata={\"help\": \"Number of evaluation time-series to use\"}\n",
        "    )\n",
        "    stride: int = field(\n",
        "        default=32, metadata={\"help\": \" Stride to segment the training data at\"}\n",
        "    )\n",
        "    training_h5_file: str = field(\n",
        "        default=None, metadata={\"help\": \"File path to the training data hdf5 file\"}\n",
        "    )\n",
        "    eval_h5_file: str = field(\n",
        "        default=None, metadata={\"help\": \"File path to the evaluation data hdf5 file\"}\n",
        "    )\n",
        "    overwrite_cache: bool = field(\n",
        "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
        "    )\n",
        "    cache_path:str= field(\n",
        "        default=None, metadata={\"help\": \"File directory to write cache file to\"}\n",
        "    )\n",
        "\n",
        "@dataclass\n",
        "class TrainingArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
        "    \"\"\"\n",
        "    block_size: int = field(\n",
        "        default=-1,\n",
        "        metadata={\n",
        "            \"help\": \"Optional input sequence length after tokenization.\"\n",
        "            \"The training dataset will be truncated in block of this size for training.\"\n",
        "            \"Default to the model max input length for single sentence inputs (take into account special tokens).\"\n",
        "        },\n",
        "    )\n",
        "    # Training paths for logging, checkpoints etc.\n",
        "    exp_dir: str = field(\n",
        "        default=None, metadata={\"help\": \"Directory to store data related to the experiment\"}\n",
        "    )\n",
        "    ckpt_dir: str = field(\n",
        "        default=None, metadata={\"help\": \"Directory to save model checkpoints during training\"}\n",
        "    )\n",
        "    plot_dir: str = field(\n",
        "        default=None, metadata={\"help\": \"Directory to save plots during training\"}\n",
        "    )\n",
        "    save_steps: int = field(\n",
        "        default=25, metadata={\"help\": \"Epoch stride to save checkpoints\"}\n",
        "    )\n",
        "    eval_steps: int = field(\n",
        "        default=25, metadata={\"help\": \"Epoch stride to evaluate validation data-set\"}\n",
        "    )\n",
        "    plot_max: int = field(\n",
        "        default=3, metadata={\"help\": \"Max number of eval cases to plot\"}\n",
        "    )\n",
        "\n",
        "    epoch_start: int = field(\n",
        "        default=0, metadata={\"help\": \"Epoch to start training at\"}\n",
        "    )\n",
        "    epochs: int = field(\n",
        "        default=200, metadata={\"help\": \"Number of epochs to train\"}\n",
        "    )\n",
        "\n",
        "    # ===== Optimization parameters =====\n",
        "    lr: float = field(\n",
        "        default=0.001, metadata={\"help\": \"Learning rate\"}\n",
        "    )\n",
        "    max_grad_norm: float = field(\n",
        "        default=0.1, metadata={\"help\": \"Norm limit for clipping gradients\"}\n",
        "    )\n",
        "    dataloader_drop_last: bool = field(\n",
        "        default=True, metadata={\"help\": \"Drop training cases no in a full mini-batch\"}\n",
        "    )\n",
        "    gradient_accumulation_steps: int = field(\n",
        "        default=int(1), metadata={\"help\": \"How many mini-batches to compute before updating weights\"}\n",
        "    )\n",
        "\n",
        "    # ===== Data loader parameters =====\n",
        "    train_batch_size: int = field(\n",
        "        default=256, metadata={\"help\": \"Number of training cases in mini-batch\"}\n",
        "    )\n",
        "    eval_batch_size: int = field(\n",
        "        default=16, metadata={\"help\": \"Number of evaluation cases in mini-batch\"}\n",
        "    )\n",
        "\n",
        "    # ===== Parallel parameters =====\n",
        "    local_rank: int = field(\n",
        "        default=-1, metadata={\"help\": \"Local rank of the CPU process, -1 means just use a single CPU\"}\n",
        "    )\n",
        "    n_gpu: int = field(\n",
        "        default=1, metadata={\"help\": \"Number of GPUs per CPU\"}\n",
        "    )\n",
        "    seed: int = field(\n",
        "        default=12345, metadata={\"help\": \"Random seed for reproducibility\"}\n",
        "    )\n",
        "    notes: str = field(\n",
        "        default=None, metadata={\"help\": \"Notes that will be appended to experiment folder\"}\n",
        "    )\n",
        "\n",
        "\n",
        "class ArgUtils:\n",
        "    \"\"\"Argument utility class for modifying particular arguments after initialization\n",
        "    \"\"\"\n",
        "    @classmethod\n",
        "    def config(\n",
        "        cls,\n",
        "        modelArgs: ModelArguments,\n",
        "        dataArgs: DataArguments,\n",
        "        trainingArgs: TrainingArguments,\n",
        "        create_paths: bool = True\n",
        "    ) -> Tuple[ModelArguments, DataArguments, TrainingArguments]:\n",
        "        \"\"\"Runs additional runtime configuration updates for argument instances\n",
        "\n",
        "        Args:\n",
        "            modelArgs (ModelArguments): Transformer model arguments\n",
        "            dataArgs (DataArguments): Data loader/ data set arguments\n",
        "            trainingArgs (TrainingArguments): Training arguments\n",
        "            create_paths (bool, optional): Create training/testing folders. Defaults to True.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[ModelArguments, DataArguments, TrainingArguments]: Updated argument instances\n",
        "        \"\"\"\n",
        "        modelArgs = cls.configModelNames(modelArgs)\n",
        "\n",
        "        if create_paths:\n",
        "            modelArgs, dataArgs, trainingArgs = cls.configPaths(modelArgs, dataArgs, trainingArgs)\n",
        "\n",
        "        trainingArgs = cls.configTorchDevices(trainingArgs)\n",
        "\n",
        "        return modelArgs, dataArgs, trainingArgs\n",
        "\n",
        "    @classmethod\n",
        "    def configModelNames(cls, modelArgs: ModelArguments) -> ModelArguments:\n",
        "        # Set up model, config, viz and embedding names\n",
        "        if not modelArgs.init_name in INITS:\n",
        "            logger.warn('Selected init name not in built-in models. Be careful.')\n",
        "\n",
        "        attribs = [\"model_name\", \"config_name\", \"embedding_name\", \"viz_name\"]\n",
        "        for attrib in attribs:\n",
        "            if getattr(modelArgs, attrib) is None:\n",
        "                setattr(modelArgs, attrib, modelArgs.init_name)\n",
        "\n",
        "        return modelArgs\n",
        "\n",
        "    @classmethod\n",
        "    def configPaths(\n",
        "        cls,\n",
        "        modelArgs: ModelArguments,\n",
        "        dataArgs: DataArguments,\n",
        "        trainingArgs: TrainingArguments\n",
        "    ) -> Tuple[ModelArguments, DataArguments, TrainingArguments]:\n",
        "        \"\"\"Sets up various folder path parameters\n",
        "\n",
        "        Args:\n",
        "            modelArgs (ModelArguments): Transformer model arguments\n",
        "            dataArgs (DataArguments): Data loader/ data set arguments\n",
        "            trainingArgs (TrainingArguments): Training arguments\n",
        "\n",
        "        Returns:\n",
        "            Tuple[ModelArguments, DataArguments, TrainingArguments]: Updated argument instances\n",
        "        \"\"\"\n",
        "        if(trainingArgs.exp_dir is None):\n",
        "            trainingArgs.exp_dir = os.path.join(HOME, 'outputs', 'transformer_{:s}'.format(modelArgs.config_name), \\\n",
        "                    'ntrain{:d}_epochs{:d}_batch{:d}'.format(dataArgs.n_train, trainingArgs.epochs, trainingArgs.train_batch_size))\n",
        "            if trainingArgs.notes: # If notes add them to experiment folder name\n",
        "                trainingArgs.exp_dir = os.path.join(os.path.dirname(trainingArgs.exp_dir), os.path.basename(trainingArgs.exp_dir)+'_{:s}'.format(trainingArgs.notes))\n",
        "\n",
        "        if(trainingArgs.ckpt_dir is None):\n",
        "            trainingArgs.ckpt_dir = os.path.join(trainingArgs.exp_dir, 'checkpoints')\n",
        "\n",
        "        if(trainingArgs.plot_dir is None):\n",
        "            trainingArgs.plot_dir = os.path.join(trainingArgs.exp_dir, 'viz')\n",
        "\n",
        "        # Create directories if they don't exist already\n",
        "        os.makedirs(trainingArgs.exp_dir, exist_ok=True)\n",
        "        os.makedirs(trainingArgs.ckpt_dir, exist_ok=True)\n",
        "        os.makedirs(trainingArgs.plot_dir, exist_ok=True)\n",
        "\n",
        "        return modelArgs, dataArgs, trainingArgs\n",
        "\n",
        "    @classmethod\n",
        "    def configTorchDevices(cls, args: TrainingArguments) -> TrainingArguments:\n",
        "        \"\"\"Sets up device ids for training\n",
        "\n",
        "        Args:\n",
        "            args (TrainingArguments): Training arguments\n",
        "\n",
        "        Returns:\n",
        "            TrainingArguments: Updated argument instance\n",
        "        \"\"\"\n",
        "        # Set up parallel PyTorch device(s)\n",
        "        if(torch.cuda.device_count() > 1 and args.n_gpu > 1):\n",
        "            if(torch.cuda.device_count() < args.n_gpu):\n",
        "                args.n_gpu = torch.cuda.device_count()\n",
        "            if(args.n_gpu < 1):\n",
        "                args.n_gpu = torch.cuda.device_count()\n",
        "            logging.info(\"Looks like we have {:d} GPUs to use. Going parallel.\".format(args.n_gpu))\n",
        "            args.device_ids = [i for i in range(0,args.n_gpu)]\n",
        "            args.src_device = \"cuda:{}\".format(args.device_ids[0])\n",
        "        # Set up parallel PyTorch single GPU device\n",
        "        elif(torch.cuda.is_available()):\n",
        "            logging.info(\"Using a single GPU for training.\")\n",
        "            args.device_ids = [0]\n",
        "            args.src_device = \"cuda:{}\".format(args.device_ids[0])\n",
        "            args.n_gpu = 1\n",
        "        # CPU only\n",
        "        else:\n",
        "            logging.info(\"No GPUs found, will be training on CPU.\")\n",
        "            args.src_device = \"cpu\"\n",
        "\n",
        "        return args"
      ],
      "metadata": {
        "id": "GdhkMnq3g5cT"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j396PtYzV_S1"
      },
      "source": [
        "## Initalizing Config and Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcpC9Fy243RN",
        "outputId": "0e8c6d5e-c109-44e4-ca04-1bfc45e236f1"
      },
      "source": [
        "# Parse arguments using the hugging face argument parser\n",
        "parser = HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n",
        "model_args, data_args, training_args = parser.parse_args_into_dataclasses(argv)\n",
        "\n",
        "# Setup logging\n",
        "#logging.basicConfig(\n",
        "#    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "#    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "#    level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN)\n",
        "# Configure arguments after intialization\n",
        "model_args, data_args, training_args = ArgUtils.config(model_args, data_args, training_args)\n",
        "\n",
        "# Rossler configuration\n",
        "config = RepressilatorConfig()\n",
        "# Load embedding model\n",
        "embedding_model = RepressilatorEmbedding(config).to(training_args.src_device)\n",
        "embedding_model.load_model(model_args.embedding_file_or_path)\n",
        "\n",
        "# Load visualization utility class\n",
        "viz = RepressilatorViz(training_args.plot_dir)\n",
        "\n",
        "# Init transformer model\n",
        "transformer = PhysformerGPT2(config, model_args.model_name)\n",
        "model  = PhysformerTrain(config, transformer)\n",
        "if(training_args.epoch_start > 0):\n",
        "    model.load_model(training_args.ckpt_dir, epoch=training_args.epoch_start)\n",
        "if(model_args.transformer_file_or_path):\n",
        "    model.load_model(model_args.transformer_file_or_path)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-d3c0922a1164>:183: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
            "  logger.warn('Selected init name not in built-in models. Be careful.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of embedding parameters: 69303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYAA52lXViUr"
      },
      "source": [
        "## Creating Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auDiMVZ5UfNz"
      },
      "source": [
        "Next create the training and validation datasets. This will probably take a little bit. We need to compute the embedded representations for the transformer for each example. Forcunately, assuming your embedding model has not changed or block size, the dataset will be locally cached allowing for fast reloading in the future. Use the \"overwrite_cache\" argument to force a new dataset creation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnrtuKdhGuWQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4249ad4-1304-4fa7-ad39-912b9736947b"
      },
      "source": [
        "# Initialize training and validation datasets\n",
        "training_data = RepressilatorDataset(\n",
        "    embedding_model,\n",
        "    data_args.training_h5_file,\n",
        "    block_size=config.n_ctx,\n",
        "    stride=data_args.stride,\n",
        "    ndata=data_args.n_train,\n",
        "    overwrite_cache=data_args.overwrite_cache)\n",
        "\n",
        "eval_data = RepressilatorDataset(\n",
        "    embedding_model,\n",
        "    data_args.eval_h5_file,\n",
        "    block_size=256,\n",
        "    stride=1024,\n",
        "    #block_size=config.n_ctx,\n",
        "    #stride=data_args.stride,\n",
        "    ndata=data_args.n_eval,\n",
        "    eval = True,\n",
        "    overwrite_cache=data_args.overwrite_cache)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-f55de6a944db>:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
            "  data_series = torch.Tensor(h5_file[key]).to(embedder.devices[0]).view([-1] + embedder.input_dims)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yKpYWniHP1D"
      },
      "source": [
        "Initialize the optimizer and scheduler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2x1UHjRHTQ3"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=training_args.lr, weight_decay=1e-8)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 14, 2, eta_min=1e-8)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFNXGEaoVovI"
      },
      "source": [
        "## Training the Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUUQltYpHbT-"
      },
      "source": [
        "Create training class and train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pwp3oOjRHd6j"
      },
      "source": [
        " trainer = Trainer(\n",
        "        model,\n",
        "        training_args,\n",
        "        (optimizer, scheduler),\n",
        "        train_dataset = training_data,\n",
        "        eval_dataset = eval_data,\n",
        "        embedding_model = embedding_model,\n",
        "        viz=viz)\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlYEwSe6WHoF"
      },
      "source": [
        "## Visualization of Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2K4s1xAXAA6"
      },
      "source": [
        "We will just embed a few of the test prediction for several epochs. For this particular attractor, it can be hit or miss because of the chaotic jumps on the z-axis. Both the embedding model and transformer training can be further fine tuned. More can be viewed in the outputs folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7J5GgEFh_8t"
      },
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "for epoch in [1, 50, 100, 150, 200]:\n",
        "  print('Validation prediction for epoch: {:d}'.format(epoch))\n",
        "  file_path = './outputs/transformer_repressilator/ntrain2048_epochs200_batch32/viz/repressilatorPred1_{:d}.png'.format(epoch)\n",
        "  display(Image(file_path, width=300, height=300))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QENxBeS-63s9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}